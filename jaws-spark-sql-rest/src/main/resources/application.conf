spray.can.server {
    # uncomment the next line for making this an HTTPS example
    # ssl-encryption = on
    idle-timeout = 301 s
    request-timeout = 300 s
  }

remote{
akka {
  //loglevel = "DEBUG"
  actor {
    provider = "akka.remote.RemoteActorRefProvider"
  }
  remote {
     enabled-transports = ["akka.remote.netty.tcp"]
    log-sent-messages = on
    log-received-messages = on
    netty.tcp {
     transport-class = "akka.remote.transport.netty.NettyTransport"
      hostname = "devbox.local"
      port = 4042
    }
  }
}
}

############ spark configuration - see spark documentation ####################
sparkConfiguration {
	spark-executor-memory=2g
	spark-mesos-coarse=false
	spark-scheduler-mode=FAIR
	spark-cores-max=2
	spark-master="spark://devbox.local:7077"
	spark-path="/home/ubuntu/latest-mssh/spark-1.1.0"
  spark-mesos-executor-home="/home/ubuntu/latest-mssh/spark-1.1.0"
	spark-default-parallelism=384
	spark-storage-memoryFraction=0.3
	spark-shuffle-memoryFraction=0.6
	spark-shuffle-compress=true
	spark-shuffle-spill-compress=true
	spark-reducer-maxMbInFlight=48
	spark-akka-frameSize=10000
	spark-akka-threads=4
	spark-akka-timeout=100
	spark-task-maxFailures=4
	spark-shuffle-consolidateFiles=true
	spark-deploy-spreadOut=true
	spark-shuffle-spill=false
 	#Serialization settings commented until more tests are performed
  	#spark-serializer="org.apache.spark.serializer.KryoSerializer"
  	#spark-kryoserializer-buffer-mb=10
  	#spark-kryoserializer-buffer-max-mb=64
  spark-kryo-referenceTracking=false


}

######### application configuration ###################
appConf{
	# the interface on which to start the spray server : localhost/ip/hostname
	server.interface=localhost
	# the cors filter allowed hosts
	cors-filter-allowed-hosts="*"
	# the default number of results retrieved on queries
	nr.of.results=100
	# the ip of the destination namenode - it is used when querying with unlimited number of results.
	rdd.destination.ip="devbox.local"
	# where to store the results in the case of an unlimited query. Possible results : hdfs/tachyon. Default hdfs
	rdd.destination.location="hdfs"
	# the remote doamain actor address
	remote.domain.actor=""
	#remote.domain.actor="devbox.local:port,devbox2.local:port"
	# application name
	application.name="Jaws"
	# the port on which to deploy the apis
	web.services.port=9080
	# the port on which to deploy the web sockets api (logs)
	web.sockets.port=8182
	# the number of threads used to execute shark commands
	nr.of.threads=10
	# implicit akka timeout
	timeout=1000000
	#where to log:  app.logging.type = cassandra/hdfs
	app.logging.type=cassandra
	# folder where to write the results schema
	schemaFolder=jawsSchemaFolder
	# the path to the xpatterns-jaws in target folder
	jar-path=/home/user/http-spark-sql-server/jaws-spark-sql-rest/target/jaws-spark-sql-rest.jar
	#jar-path=/home/user/http-spark-sql-server/jaws-spark-sql-rest/target/test-app.jar
	# the path to the hdfs namenode
	hdfs-namenode-path="hdfs://devbox.local:8020"
	# the path to the tachyon namenode
	tachyon-namenode-path="tachyon://devbox.local:19998"
}

########## hadoop configuration - skip this if you are using cassandra logging ########
hadoopConf {
	namenode="hdfs://devbox.local:8020"
	replicationFactor=1
	# set on true if you want to start fresh (all the existing folders will be recreated)
	forcedMode=false
	# folder where to write the logs
	loggingFolder=jawsLogs
	# folder where to write the jobs states
	stateFolder=jawsStates
	# folder where to write the jobs details
	detailsFolder=jawsDetails
	# folder where to write the jobs results
	resultsFolder=jawsResultsFolder
	# folder where to write the jobs meta information
	metaInfoFolder=jawsMetainfoFolder
	# folder where to write the jobs execution time
	executionTimeFolder=jawsExecutionTimeFolder
	# folder where to write the parquet tables information
	parquetTablesFolder=parquetTablesFolder
}

########## cassandra configuration - skip this if you are using hdfs logging ##########
cassandraConf {
	cassandra.host="devbox.local:9160"
	cassandra.keyspace=xpatterns_jaws
	cassandra.cluster.name=Jaws
}


